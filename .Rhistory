### Load Sales Data
geosales <- read.csv (file = '/Users/john/geodata/geosales.csv', colClasses = c(date="Date", geo="character", var1="numeric", var2="numeric", var3="numeric"))
### Load Packages
library(tidyverse)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign.R')
View(pre.summary)
View(geoassignts)
View(geoassign)
View(pre.predict)
View(pre.summary)
plot(pre.summary)
plot(pre.predict)
plot(geosales)
summary(geosales)
summary(geoassign)
plot(summary(geoassign))
View(geoassign)
system.time()
system.time()
sys.time()
Sys.Date
Sys.Date()
write_csv(geoassign, file = paste('geoassign.csv', Sys.Date())
write_csv(geoassign, file = paste("geoassign.csv", Sys.Date())
write_csv(geoassign, file = 'geoassign.csv')
getwd()
write_csv(geoassign, file = paste(Sys.Date(), "geoassign.csv", sep = " ")
write_csv(geoassign, file = paste(Sys.Date(), "geoassign", sep = " ")
write_csv(geoassign, file = paste(Sys.Date(), "geoassign", sep = " ")
csvfile <- paste(Sys.Date(), "geoassign", sep = " ")
csvfile <- paste(Sys.Date(), "geoassign", sep = " ")
csvfile <- paste(Sys.Date(), "geoassign.csv", sep = " ")
write_csv(geoassign, file = csvfile)
csvfilename <- paste(Sys.Date(), "geoassign.csv", sep = " ")
write_csv(geoassign, file = csvfilename)
rm(csvfile)
View(pre.predict)
View(geosales)
View(geoassignts)
View(geoassign)
View(pre.summary)
install.packages("devtools")
devtools::install_github("google/GeoexperimentsResearch")
devtools::install_github("twitter/AnomalyDetection")
getwd()
paste(getwd(), "/geosales.csv")
paste0(getwd(), "/geosales.csv")
csv.importfn <- paste0(getwd(), "/geosales.csv")
geosales <- read.csv (file = csv.importfn, colClasses = c(date="Date", geo="character", var1="numeric", var2="numeric", var3="numeric"))
geosales <- read.csv (file = csv.importfn, colClasses = c(date="Date", geo="character", var1="numeric", var2="numeric", var3="numeric"))
GeoTimeseries(geosales, metrics="var1")
library(tidyverse)
library(purrr)
library(tidyr)
library(GeoexperimentsResearch)
library (AnomalyDetection)
GeoTimeseries(geosales, metrics="var1")
?ExtractGeoStrata
geoassign <- Randomize(ExtractGeoStrata(GeoTimeseries(geosales, metrics="var1"), volume="var1", n.groups=2))
View(geoassign)
csv.importfn <- paste0(getwd(), "/geosales.csv") #Sets csv location and name to current working directory. Typically this is the root directy of the script.
geosales <- read.csv (file = csv.importfn, colClasses = c(date="Date", geo="character", var1="numeric"))
geoassign <- Randomize(ExtractGeoStrata(GeoTimeseries(geosales, metrics="var1"), volume="var1", n.groups=2))
View(geoassign)
csv.exportfn <- paste(Sys.Date(), "geoassign.csv", sep = " ") #Export file name today's date and geoassign
write_csv(geoassign, file = csv.exportfn) #Export csv
?Randomize
summarize(geosales)
?summarize
summarize(geosales, sum("var1"))
summarize(geosales, sum(var1))
geosales %>% select (geo, var1) group_by(geo) %>% summarize(var1)
geosales %>% select (geo, var1) %>% group_by(geo) %>% summarize(var1)
geosales %>% select (geo, var1) %>% group_by(geo) %>% summarize(sum(var1))
geosales %>% select (geo, var1) %>% group_by(geo) %>% summarize(sum(var1))
geosalessummary <- geosales %>% select (geo, var1) %>% group_by(geo) %>% summarize(sum(var1))
View(geosalessummary)
randomize(geosalessummary)
Randomize(geosalessummary)
library(GeoexperimentsResearch)
View(geosalessummary)
geosalessummary <- geosalessummary %>% rename(sum(var1) = volume)
geosalessummary <- geosalessummary %>% rename("sum(var1)" = volume)
geosalessummary <- geosalessummary %>% rename('sum(var1)' = volume)
geosalessummary <- geosalessummary %>% rename('sum(var1)' = 'volume')
Randomize(geosalessummary)
rm(geoassign)
rm(geosales, geosalessummary, csv.exportfn, csv.importfn)
library (AnomalyDetection)
csv.importpath <- paste0(getwd(), "/geosales.csv") #Sets csv location and name to current working directory. Typically this is the root directy of the script.
geosales <- read.csv (file = csv.importpath, colClasses = c(date="Date", geo="character", var1="numeric"))
?AnomalyDetectionTs()
AnomalyDetectionTs(geosales)
?subset
geosales %>% select (date, var1) %>% group_by(date) %>% summarize(sum(var1))
geosalestrend <- > geosales %>% select (date, var1) %>% group_by(date) %>% summarize(sum(var1))
geosalestrend <- geosales %>% select (date, var1) %>% group_by(date) %>% summarize(sum(var1))
View(geosalestrend)
colnames(geosalestrend) c("date, var1_sum")
colnames(geosalestrend) <- c("date, var1_sum")
colnames(geosalestrend) <- c("date", "var1_sum")
AnomalyDetectionTs(geosalestrend)
AnomalyDetectionVec(geosalestrend)
?AnomalyDetectionTs
data("raw_data")
force(raw_data)
raw data <- data("raw_data")
raw data <- force("raw_data")
raw data <- data(raw_data)
raw data <- force(raw_data)
raw_data <- force(raw_data)
geosalestrend2 <- geosalestrend(as.POSIXct(date), "var1_sum")
geosalestrend2 <- geosalestrend
geosalestrend2$date <- as.POSIXct(geosalestrend2$date)
AnomalyDetectionVec(geosalestrend2)
AnomalyDetectionTs(geosalestrend2)
AnomalyDetectionTs(raw_data)
anomalies = AnomalyDetectionTs(geosalestrend, direction="pos", plot=TRUE)
library (AnomalyDetection)
anomalies = AnomalyDetectionTs(geosalestrend, direction="pos", plot=TRUE)
devtools::install_github("business-science/anomalize")
library(anomalize)
?time_decompose()
time_decompose(geosalestrend, "var1_sum")
time_decompose(geosalestrend, "var1_sum", method = c("twitter"))
library (anomalize)
library(tibbletime)
plot(time_decompose(geosalestrend, "var1_sum", method = c("twitter")))
geosalestrendanomalized <- geosalestrend %>%
time_decompose(var1_sum, merge = TRUE) %>%
anomalize(remainder) %>%
time_recompose()
View(geosalestrendanomalized)
plot_anomalies(geosalestrendanomalized)
geosalestrendanom <- geosalestrend %>% time_decompose(var1_sum, merge = TRUE) %>% anomalize(remainder) %>% time_recompose()
plot_anomalies(geosalestrendanom)
geosalestrendanom <- geosales %>% time_decompose(var1, merge = TRUE) %>% anomalize(remainder) %>% time_recompose()
plot_anomaly_decomposition(geosalestrendanom)
install.packages("devtools")
install.packages("devtools")
devtools::install_github("business-science/anomalize")
library(tidyverse)
library(purrr)
library(tidyr)
library(tibbletime)
library(GeoexperimentsResearch)
library (anomalize)
csv.importpath <- paste0(getwd(), "/geotsvar1.csv") #Sets csv location and name to current working directory. Typically this is the root directy of the script.
geosales <- read.csv (file = csv.importpath, colClasses = c(date="Date", geo="character", var1="numeric"))
geosalestrend <- geosales %>% select (date, var1) %>% group_by(date) %>% summarize(sum(var1)) #summarize var1 by date
colnames(geosalestrend) <- c("date", "var1_sum") #clean up column names
geosalestrendanom <- geosalestrend %>% time_decompose(var1_sum, merge = TRUE) %>% anomalize(remainder) %>% time_recompose()
plot_anomalies(geosalestrendanom)
plot_anomaly_decomposition(geosalestrendanom)
View(geosalestrendanom)
geoassign <- Randomize(ExtractGeoStrata(GeoTimeseries(geosales, metrics="var1"), volume="var1", n.groups=2))
View(geoassign)
geoassignts <- GeoTimeseries(geosales, metrics="var1")
View(geoassignts)
View(geoassignts)
geosalesnts <- GeoTimeseries(geosales, metrics="var1")
pre.predict <- DoROASPreanalysis(geosalesnts, response="var1", geos=geoassign, prop.to="var1", period.lengths=c(20, 15, 7))
pre.summary <- summary(pre.predict, level=0.90, type="one-sided", precision=1.0)
View(pre.predict)
View(pre.summary)
geosalesnts <- GeoTimeseries(geosales, metrics="var1")
pre.predict <- DoROASPreanalysis(geosalesnts, response="var1", geos=geoassign, prop.to="var1", period.lengths=c(20, 15, 7))
pre.summary <- summary(pre.predict, level=0.90, type="one-sided", precision=1.0)
view(pre.summary)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
plot(geosalestrend)
hist(geosalestrend)
hist(geosalestrend$var1_sum)
?plot
hist(geosalestrend$var1_sum, geosalestrend$date, type = h)
hist(geosalestrend$var1_sum, geosalestrend$date, type = "H")
hist(geosalestrend$var1_sum, geosalestrend$date, type = 'h')
hist(geosalestrend$var1_sum, type = 'h')
plot(geosalestrend$date, geosalestrend$var1_sum)
plot(geosalestrend$date, geosalestrend$var1_sum, type = 'h')
csv.importpath <- paste0(getwd(), "/geomultivar.csv") #Sets csv location and name to current working directory. Typically this is the root directy of the script.
geodata <- read.csv (file = csv.importpath, colClasses = c(date="Date", geo="character", var1="numeric", var2="numeric", var3="numeric"))
csv.importpath <- paste0(getwd(), "/geomultivar.csv") #Sets csv location and name to current working directory. Typically this is the root directy of the script.
geodata <- read.csv (file = csv.importpath, colClasses = c(geo="character", var1="numeric", var2="numeric", var3="numeric"))
View(geodata)
bin(geodata, nbins = 5, labels = NULL, method = c("length"), na.omit = TRUE)
install_github("vonjd/OneR")
library(devtools)
install_github("vonjd/OneR")
libarary(OneR)
library(OneR)
bin(geodata, nbins = 5, labels = NULL, method = c("length"), na.omit = TRUE)
optbin(geodata)
?optbin
head(iris)
optbin(iris)
OneR(geodata)
geodata2 <- geodata c("var1","var2","var3","geo")
geodata2 = geodata c("var1","var2","var3","geo")
optbin(geodata, formula = "var1")
data("breastcancer")
force(breastcancer)
geodata2 = geodata['var1', 'var2', 'var3', 'geo']
geodata2 = geodata[['var1', 'var2', 'var3', 'geo']]
geodata[['var1', 'var2', 'var3', 'geo']]
geodata2 <- geodata[, c(2, 3, 4, 1)]
optbin(geodata2)
optbindata <- optbin(geodata2)
head (optbindata)
View(optbindata)
optbin(geodata2)
summary (optbindata)
model <- OneR(geodata2, verbose = TRUE)
bin(geodata2)
bingeo = bin(geodata2)
View(bingeo)
View(bingeo, n= 10)
bingeo = bin(geodata2, n=10)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
fviz_nbclust(geodata2, kmeans, method = "wss") #ID an appropriate number of centers
fviz_cluster(k.output, data = geodata2)
?fviz_cluster
View(k.output)
k.output
view(k.output)
?kmeans
View(k.geooutput)
?factoextra
?fviz_pca_var
fviz_pca_var(geodata2)
library(FactoMineR) # stats pacakge
?PCA
pca(geodata2)
PCA(geodata2)
pcatable <- PCA(geodata)
pcatable <- PCA(geodata2)
View(pcatable)
pcatable <- PCA(geodata2, scale.unit = TRUE, graph= TRUE)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
View(k.geooutput)
fviz_cluster(k.output, data = geodata2)
?PCA
temp <- get_dist(geodata2, stand = TRUE)
temp
fviz_dist(get_dist(geodata2, stand = TRUE)) # visualize geo distance relationship
temp <- as.data.frame(get_dist(geodata2, stand = TRUE))
temp <- as.data.frame(get_dist(geodata2, stand = TRUE))
?subset
temp <- as.vector(get_dist(geodata2, stand = TRUE))
temp2 <- as.vector(get_dist(geodata2, stand = TRUE))
temp2
temp2 <- (get_dist(geodata2, stand = TRUE))
rm(temp2)
temp <- (get_dist(geodata2, stand = TRUE))
temp
?get_dist
dist(geodata2)
dist(geodata2, stand = TRUE)
as.data.frame(temp)
as.data.frame.array(temp)
get_clust_tendency(geodata2)
get_clust_tendency(geodata2, n=50)
geodata2[-3]
get_clust_tendency(geodata2, n=50, seed=987)
get_clust_tendency(geodata2[-3], n=50, seed=987)
get_clust_tendency(geodata2[-3], n=50)
get_clust_tendency(geodata2[-3], n=50)
get_clust_tendency(geodata2[-3], n=5)
View(geoassign)
hcut(geodata2, k=5, stand = TRUE)
hcutdata <- hcut(geodata2, k=5, stand = TRUE)
View(hcutdata)
fviz_dend(hcutdata, rect = TRUE, cex = 0.5)
?fviz_dend
h.output <- hcut(geodata2, k=5, stand = TRUE)
fviz_dend(h.output)
fviz_dend(h.output, rect = TRUE, cex = .3)
fviz_dend(h.output, rect = TRUE, cex = .4)
fviz_dend(h.output, rect = TRUE, cex = .7)
View(h.output)
h.output <- hcut(geodata2, k=5, stand = TRUE)
h.geoassign <- as.data.frame(h.output[["cluster"]])
fviz_dend(h.output, rect = TRUE, cex = .7)
View(h.geoassign)
h.geooutput <- merge(rownames_to_column(h.geoassign),rownames_to_column(geodata2))
View(h.output)
rm(hcutdata)
View(k.geooutput)
View(h.geooutput)
h.geoassign <- as.data.frame(h.output[["cluster"]])
names(h.geoassign)[1] <- "cluster"
h.geooutput <- merge(rownames_to_column(h.geoassign),rownames_to_column(geodata2))
merge(h.geooutput, k.geooutput)
temp <- merge(k.geooutput, h.geooutput)
temp
?merge?
?merge
?merge
View(h.geooutput)
View(k.geooutput)
temp <- merge(k.geooutput, h.geooutput, by.x = "rowname", by.y = "rowname")
View(temp)
pcatable <- PCA(geodata2, scale.unit = TRUE, graph= TRUE) # principal component analysis for variable reduction
View(pcatable)
?PCA
plot(pcatable)
plot.PCA(pcatable)
View(pcatable)
pcatable[["eig"]]
plot(pcatable[["eig"]])
view(pcatable[["eig"]])
?PCA
fviz_nbclust(geodata2, kmeans, method = "wss") # decision centers variable in clustering script
?hcut
fviz_cluster(k.output, data = geodata2) # two dimension visual of spacial clusters by principal variables
fviz_dend(h.output, rect = TRUE, cex = .7)
?fviz_dend
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
View(k.h.geooutput)
k.output <- kmeans(geodata2, centers = 5, nstart = 25) # Set centers and run kmeans model
k.geoassign <- as.data.frame(k.output[["cluster"]])
names(k.geoassign)[1] <- "cluster"
k.geooutput <- merge(rownames_to_column(k.geoassign),rownames_to_column(geodata2))
fviz_cluster(k.output, data = geodata2) # two dimension visual of spacial clusters by principal variables
h.output <- hcut(geodata2, k=5, stand = TRUE)
h.geoassign <- as.data.frame(h.output[["cluster"]])
names(h.geoassign)[1] <- "cluster"
fviz_dend(h.output, rect = TRUE, cex = .7)
h.geooutput <- merge(rownames_to_column(h.geoassign),rownames_to_column(geodata2))
k.h.geooutput <- merge(k.geooutput, h.geooutput, by.x = "rowname", by.y = "rowname") # compare partitioning and hierarchical clustering assignments by geo
View(s.geoassign)
?kmeans
?hcut
?kmeans
?hcut
csv.exportfn <- paste(Sys.Date(), "geoassign_noTS.csv", sep = " ") # Export file name today's date and geoassign_noTS
write_csv(h.geoassign, file = csv.exportfn) # Export csv in the working direct, x= (s.geoassign or k.geoassign or h.geoassign)
View(s.geoassign)
?cluster
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
library(GeoexperimentsResearch) #core geo experiment functions
library(tidyverse)  # data manipulation
library (anomalize) #anomaly id functions
csv.importpath <- paste0(getwd(), "/geotsvar1.csv") #Sets csv location and name to current working directory. Typically this is the root directory of the script.
geotsvar1 <- read.csv (file = csv.importpath, colClasses = c(date="Date", geo="character", var1="numeric"))
geosalestrend <- geotsvar1 %>% select (date, var1) %>% group_by(date) %>% summarize(sum(var1)) #summarize var1 by date
View(geosalestrend)
colnames(geosalestrend) <- c("date", "var1_sum") #clean up column names
geosalestrendanom <- geosalestrend %>% time_decompose(var1_sum, merge = TRUE) %>% anomalize(remainder) %>% time_recompose() #anomoly detection and flagging in the dataframe
plot_anomalies(geosalestrendanom) #simple plot of the anomalous observations
View(geosalestrendanom)
plot_anomaly_decomposition(geosalestrendanom)
?time_decompose
View(geosalestrendanom)
time_decompose(var1_sum, merge = TRUE)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (TS).R')
View(geosalestrendanom)
detach("package:cluster", unload = TRUE)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
View(h.geooutput)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
fviz_dend(h.output, rect = TRUE, cex = .7)
?fviz_dend
fviz_dend(h.output, rect = TRUE, cex = .7, horiz = TRUE)
source('~/Documents/GeoExperiments/Scripts/GeoExperiments_Assign (noTS).R')
fviz_nbclust(geodata2, kmeans, method = "wss") # decision clusters variable in scripts below
# Prelim analysis
pcatable <- PCA(geodata2, scale.unit = TRUE) # principal component analysis for variable reduction
# Prelim analysis
pcatable <- PCA(geodata2, scale.unit = TRUE) # principal component analysis for variable reduction
view(pcatable[["eig"]]) # variable weight summary from PCA analysis
fviz_nbclust(geodata2, kmeans, method = "wss") # decision clusters variable in scripts below
# Hierarchical cluster assignment (h.geooutput)
h.output <- hcut(geodata2, k=5, stand = TRUE) # configure clusters(k) to target clusters from fviz_nbclust
### Load Packages
library(GeoexperimentsResearch) #simple geo assignment functions
library(tidyverse)  # data manipulation
library(factoextra) # clustering algorithms & visualization
library(FactoMineR) # stats package
#library(cluster)    # clustering algorithms
### Load Sales Data
csv.importpath <- paste0(getwd(), "/geomultivar.csv") #Sets csv location and name to current working directory. Typically this is the root directory of the script.
geodata <- read.csv (file = csv.importpath, colClasses = c(geo="character", var1="numeric", var2="numeric", var3="numeric"))
geodata2 <- read.csv (file = csv.importpath, row.names = 1, colClasses = c(geo="character", var1="numeric", var2="numeric", var3="numeric"))
s.geooutput <- Randomize(GeoStrata(Geos(geodata, volume = 'var1'), n.groups = 4)) #stratifies by volume, strata sizes of n.groups, and randomly assigns 1:n.groups to each geo within a strata
View(s.geooutput)
getwd()
library(GeoexperimentsResearch)
csv.geo <- paste0(getwd(), "/geoassignment.csv")
csv.sc <- paste0(getwd(), "/salesandcost.csv")
csv.geo <- paste0(getwd(), "/geoassignment.csv")
csv.sc <- paste0(getwd(), "/salesandcost.csv")
geoassignment <- read.csv (file = csv.geo,
colClasses = c(geo="character", geo.group="integer"))
salesandcost <- read.csv (file = csv.sc,
colClasses = c(date="Date", geo="character", sales="numeric", cost="numeric"))
geosalestrend <- geotsvar1 %>% select (date, var1) %>% group_by(date) %>% summarize(sum(var1)) #summarize var1 by date
geosalestrend <- salesandcost %>% select (date, sales) %>% group_by(date) %>% summarize(sum(sales)) #sum sales by date
library(tidyverse)  # data manipulation
geosalestrend <- salesandcost %>% select (date, sales) %>% group_by(date) %>% summarize(sum(sales)) #sum sales by date
View(geosalestrend)
colnames(geosalestrend) <- c("date", "sales") #clean up column names
geosalestrend <- salesandcost %>% select (date, sales) %>% group_by(date) %>% summarize(sum(sales)) #sum sales by date
colnames(geosalestrend) <- c("date", "sales") #clean up column names
geosalestrendanom <- geosalestrend %>% time_decompose(sales, merge = TRUE) %>% anomalize(remainder) %>% time_recompose() #anomoly detection and flagging in the dataframe
plot_anomalies(geosalestrendanom) #simple plot of the anomalous observations
plot_anomaly_decomposition(geosalestrendanom) #more detailed plot including underlying trends
library (anomalize) #anomaly id functions
geosalestrend <- salesandcost %>% select (date, sales) %>% group_by(date) %>% summarize(sum(sales)) #sum sales by date
colnames(geosalestrend) <- c("date", "sales") #clean up column names
geosalestrendanom <- geosalestrend %>% time_decompose(sales, merge = TRUE) %>% anomalize(remainder) %>% time_recompose() #anomoly detection and flagging in the dataframe
plot_anomalies(geosalestrendanom) #simple plot of the anomalous observations
plot_anomaly_decomposition(geosalestrendanom) #more detailed plot including underlying trends
periods = ExperimentPeriods(
period.dates = c("2015-01-05", "2015-02-16", "2015-03-15", "2015-04-07"),
period.names = c("PreTest", "Test", "Cooldown"))
data = GeoExperimentData(GeoTimeseries(salesandcost, metrics=c("sales", "cost")), periods=periods, geo.assignment=GeoAssignment(geoassignment))
GEOresult = DoGBRROASAnalysis(data, response='sales', cost='cost', pretest.period=0, intervention.period=1, cooldown.period=2, control.group=1, treatment.group=2)
TSresult = DoTBRAnalysis(data, response='sales', model='tbr1', pretest.period=0, intervention.period=1, cooldown.period=2, control.group=1, treatment.group=2)
TSROASresult = DoTBRROASAnalysis(data, response='sales', cost='cost', model='tbr1', pretest.period=0, intervention.period=1, cooldown.period=2, control.group=1, treatment.group=2)
Summary (TSresult) #Simple summary of time series regression valuation
Summary (GEOresult, level=0.90, threshold=3.0, interval.type="two-sided") #Results summary with configurable parameters
Summary (TSresult) #Simple summary of time series regression valuation
Summary(TSresult) #Simple summary of time series regression valuation
library(purrr)
Summary(TSresult)
library(tidyr)
summary(TSresult)
Summary(GEOresult, level=0.90, threshold=3.0, interval.type="two-sided") #Results summary with configurable parameters
summary(GEOresult, level=0.90, threshold=3.0, interval.type="two-sided") #Results summary with configurable parameters
plot(TSresult)
aggregate(data, by=c('period', 'geo.group'))
view(aggregate(data, by=c('period', 'geo.group')))
View(GEOresult)
GEOresult = DoGBRROASAnalysis(data, response='sales', cost='cost', pretest.period=0, intervention.period=1, cooldown.period=2, control.group=1, treatment.group=2)
TSresult = DoTBRAnalysis(data, response='sales', model='tbr1', pretest.period=0, intervention.period=1, cooldown.period=2, control.group=1, treatment.group=2)
TSROASresult = DoTBRROASAnalysis(data, response='sales', cost='cost', model='tbr1', pretest.period=0, intervention.period=1, cooldown.period=2, control.group=1, treatment.group=2)
summary(TSresult) #Simple summary of time series regression valuation
summary(GEOresult, level=0.90, threshold=3.0, interval.type="two-sided") #Results summary with configurable parameters
summary(TSROASresult)
View(GEOresult)
library(GeoexperimentsResearch) #core geo experiment functions
library(tidyverse)  # data manipulation
library (anomalize) #anomaly id functions
csv.importpath <- paste0(getwd(), "/geotsvar1.csv") #Sets csv location and name to current working directory. Typically this is the root directory of the script.
geotsvar1 <- read.csv (file = csv.importpath, colClasses = c(date="Date", geo="character", var1="numeric"))
geosalestrend <- geotsvar1 %>% select (date, var1) %>% group_by(date) %>% summarize(sum(var1)) #summarize var1 by date
colnames(geosalestrend) <- c("date", "var1_sum") #clean up column names
geosalestrendanom <- geosalestrend %>% time_decompose(var1_sum, merge = TRUE) %>% anomalize(remainder) %>% time_recompose() #anomoly detection and flagging in the dataframe
plot_anomalies(geosalestrendanom) #simple plot of the anomalous observations
plot_anomaly_decomposition(geosalestrendanom) #more detailed plot including underlying trends
geoassign <- Randomize(ExtractGeoStrata(GeoTimeseries(geotsvar1, metrics="var1"), volume="var1", n.groups=4)) #stratifies by volume, strata sizes of n.groups, and randomly assigns 1:n.groups to each geo within a strata
csv.exportfn <- paste(Sys.Date(), "geoassignTS.csv", sep = " ") #Export file name today's date and geoassignTS
write_csv(geoassign, file = csv.exportfn) #Export csv in the working direct
library(GeoexperimentsResearch) #simple geo assignment functions
library(tidyverse)  # data manipulation
library(factoextra) # clustering algorithms & visualization
library(FactoMineR) # stats package
csv.importpath <- paste0(getwd(), "/geomultivar.csv") #Sets csv location and name to current working directory. Typically this is the root directory of the script.
geodata <- read.csv (file = csv.importpath,
colClasses = c(geo="character", var1="numeric", var2="numeric", var3="numeric"))
geodata2 <- read.csv (file = csv.importpath, row.names = 1,
colClasses = c(geo="character", var1="numeric", var2="numeric", var3="numeric"))
s.geooutput <- Randomize(GeoStrata(Geos(geodata, volume = 'var1'), n.groups = 4)) #stratifies by volume, strata sizes of n.groups, and randomly assigns 1:n.groups to each geo within a strata
pcatable <- PCA(geodata2, scale.unit = TRUE) # principal component analysis for variable reduction
view(pcatable[["eig"]]) # variable weight summary from PCA analysis
pcatable <- PCA(geodata2, scale.unit = TRUE, graph = FALSE) # principal component analysis for variable reduction
view(pcatable[["eig"]]) # variable weight summary from PCA analysis
fviz_nbclust(geodata2, kmeans, method = "wss") # decision clusters variable in scripts below
csv.exportfn <- paste(Sys.Date(), "geoassign_noTS.csv", sep = " ") # Export file name today's date and geoassign_noTS
write_csv(s.geooutput, file = csv.exportfn) # Export csv in the working direct, x= (s.geooutput or k.geooutput or h.geooutput)
k.output <- kmeans(geodata2, centers = 5, nstart = 25) # configure centers to target clusters from fviz_nbclust
k.geoassign <- as.data.frame(k.output[["cluster"]])
names(k.geoassign)[1] <- "cluster"
k.geooutput <- merge(rownames_to_column(k.geoassign),rownames_to_column(geodata2)) #combine original dataset and custer assignments
fviz_cluster(k.output, data = geodata2) # two dimension visual of spacial clusters of the top two principal variables
h.output <- hcut(geodata2, k=5, stand = TRUE) # configure clusters(k) to target clusters from fviz_nbclust
h.geoassign <- as.data.frame(h.output[["cluster"]])
names(h.geoassign)[1] <- "cluster"
fviz_dend(h.output, rect = TRUE, cex = .7, horiz = TRUE) # dendrogram of geo clusters and relative distance
h.geooutput <- merge(rownames_to_column(h.geoassign),rownames_to_column(geodata2)) #combine original dataset and custer assignments
k.h.geooutput <- merge(k.geooutput, h.geooutput, by.x = "rowname", by.y = "rowname") # compare partitioning and hierarchical clustering assignments by geo
### Export Results
csv.exportfn <- paste(Sys.Date(), "geoassign_noTS.csv", sep = " ") # Export file name today's date and geoassign_noTS
write_csv(h.geooutput, file = csv.exportfn) # Export csv in the working direct, x= (s.geooutput or k.geooutput or h.geooutput)
fviz_dist(get_dist(geodata2, stand = TRUE)) # visualize geo distance relationship
install.packages(anomolize)
